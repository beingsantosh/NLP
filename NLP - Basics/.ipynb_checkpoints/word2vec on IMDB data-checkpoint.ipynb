{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing word2vec pre train model from gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from en_core_web_sm==2.3.1) (2.3.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.50.2)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.0.post20201006)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'dog cat lion ghjg'\n",
    "doc = nlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True (300,) [-4.0176e-01  3.7057e-01  2.1281e-02 -3.4125e-01  4.9538e-02  2.9440e-01\n",
      " -1.7376e-01 -2.7982e-01  6.7622e-02  2.1693e+00 -6.2691e-01  2.9106e-01\n",
      " -6.7270e-01  2.3319e-01 -3.4264e-01  1.8311e-01  5.0226e-01  1.0689e+00\n",
      "  1.4698e-01 -4.5230e-01 -4.1827e-01 -1.5967e-01  2.6748e-01 -4.8867e-01\n",
      "  3.6462e-01 -4.3403e-02 -2.4474e-01 -4.1752e-01  8.9088e-02 -2.5552e-01\n",
      " -5.5695e-01  1.2243e-01 -8.3526e-02  5.5095e-01  3.6410e-01  1.5361e-01\n",
      "  5.5738e-01 -9.0702e-01 -4.9098e-02  3.8580e-01  3.8000e-01  1.4425e-01\n",
      " -2.7221e-01 -3.7016e-01 -1.2904e-01 -1.5085e-01 -3.8076e-01  4.9583e-02\n",
      "  1.2755e-01 -8.2788e-02  1.4339e-01  3.2537e-01  2.7226e-01  4.3632e-01\n",
      " -3.1769e-01  7.9405e-01  2.6529e-01  1.0135e-01 -3.3279e-01  4.3117e-01\n",
      "  1.6687e-01  1.0729e-01  8.9418e-02  2.8635e-01  4.0117e-01 -3.9222e-01\n",
      "  4.5217e-01  1.3521e-01 -2.8878e-01 -2.2819e-02 -3.4975e-01 -2.2996e-01\n",
      "  2.0224e-01 -2.1177e-01  2.7184e-01  9.1703e-02 -2.0610e-01 -6.5758e-01\n",
      "  1.8949e-01 -2.6756e-01  9.2639e-02  4.3316e-01 -4.8868e-01 -3.8309e-01\n",
      " -2.1910e-01 -4.4183e-01  9.8044e-01  6.7423e-01  8.4003e-01 -1.8169e-01\n",
      "  1.7385e-01  4.1848e-01  1.6098e-01 -1.0490e-01 -4.1965e-01 -3.5660e-01\n",
      " -1.6837e-01 -6.3458e-01  3.8422e-01 -3.5043e-01  1.7486e-01  5.3528e-01\n",
      "  2.0143e-01  3.7877e-02  4.7105e-01 -4.4344e-01  1.6840e-01 -1.6685e-01\n",
      " -2.4022e-01 -1.0077e-01  3.0334e-01  4.2730e-01  3.3803e-01 -4.3481e-01\n",
      "  1.1343e-01  6.1958e-02  6.1808e-02 -1.4007e-01  8.2018e-02 -3.9130e-02\n",
      "  5.1442e-02  2.8725e-01  5.8025e-01 -5.7641e-01 -3.4652e-01  1.0132e-01\n",
      "  1.4463e-01  1.1569e-02 -3.3701e-01 -1.7586e-01 -3.5724e-01 -2.1423e-01\n",
      "  1.1429e-02  4.7645e-01 -3.7463e-02 -2.9488e-01 -1.7465e-01  3.0255e-01\n",
      "  6.0317e-01 -6.6790e-02 -2.7050e+00 -7.0308e-01  4.0548e-01  6.2874e-01\n",
      "  6.3080e-01 -5.4513e-01 -9.6191e-03  2.6533e-01  2.3391e-01 -5.1886e-02\n",
      " -6.5759e-03  1.8573e-02 -4.5693e-01 -7.0351e-02 -3.0621e-01 -1.4018e-02\n",
      " -2.0408e-01  3.7100e-01 -3.2354e-01 -8.4646e-01  2.7092e-01 -1.1961e-01\n",
      " -9.5576e-02 -6.0464e-01  4.2409e-02  2.4656e-01  3.8445e-02 -2.5467e-02\n",
      " -9.2908e-02 -2.1356e-01  3.6120e-01  1.9113e-02  6.2741e-02 -1.3083e-01\n",
      " -1.5146e-03  5.8238e-01 -1.8956e-01  7.8105e-01  1.0477e-02  1.0928e+00\n",
      "  1.0140e-01 -3.6248e-01 -1.1962e-01 -3.4462e-01 -5.5704e-01  2.5797e-01\n",
      "  3.3356e-01  3.3194e-01 -3.1298e-01 -7.5547e-01 -7.5290e-01 -9.3072e-02\n",
      " -1.1173e-01 -5.7251e-01  1.6639e-01  6.3579e-01  2.4006e-01 -2.9211e-01\n",
      "  9.0182e-01  1.2425e-01 -5.7751e-01  4.7986e-02 -4.2748e-01  2.4446e-01\n",
      "  4.7232e-02  3.5694e-01  4.4241e-01 -2.3055e-01  6.6037e-01 -7.3983e-03\n",
      " -3.7857e-01  2.2759e-01 -3.7138e-01  3.1055e-01 -7.2105e-02 -2.4490e-01\n",
      " -3.9761e-02  5.3650e-01 -4.1478e-01  1.6563e-01  3.3707e-01  1.0920e-01\n",
      "  3.7219e-01 -5.5727e-01 -7.8060e-01  1.4251e-01 -3.5828e-01  4.1638e-01\n",
      "  2.1446e-01  1.8410e-01 -4.7704e-01 -2.2005e-02 -2.3634e-01 -2.2840e-01\n",
      "  3.4722e-01  2.3667e-01  7.4249e-02 -8.8416e-02  2.8618e-01 -4.6942e-01\n",
      " -4.3914e-01 -2.6474e-01 -3.0690e-01 -1.5260e-01 -8.4870e-02  2.8410e-01\n",
      " -1.8481e-01 -2.2122e-01 -1.1169e-01 -2.5241e-02  4.5968e-02  3.5343e-02\n",
      "  2.2467e-01  5.1556e-01 -6.5137e-04  9.9559e-02 -1.4215e-01  2.0136e-01\n",
      "  2.8334e-01 -2.8772e-01  3.7766e-02 -3.7608e-01 -1.1681e-01 -6.7020e-01\n",
      " -4.6265e-02  3.8784e-01 -3.2295e-02 -5.4291e-02 -4.5384e-01  1.9552e-01\n",
      " -2.9470e-01  8.5009e-01  1.0345e-01  9.7010e-02  1.1339e-01  3.9502e-01\n",
      "  5.9043e-02  2.1978e-01  1.8845e-01 -1.5891e-01 -1.0301e-01  3.3164e-01\n",
      "  6.1477e-02 -2.9848e-01  4.4510e-01  4.7329e-01  2.6312e-01 -1.8495e-01\n",
      "  1.4652e-01 -3.1510e-02  2.2908e-02 -2.5929e-01 -3.0862e-01  1.7545e-03\n",
      " -1.8962e-01  5.4789e-01  3.1194e-01  2.4693e-01  2.9929e-01 -7.4861e-02]\n",
      "cat True (300,) [-0.15067   -0.024468  -0.23368   -0.23378   -0.18382    0.32711\n",
      " -0.22084   -0.28777    0.12759    1.1656    -0.64163   -0.098455\n",
      " -0.62397    0.010431  -0.25653    0.31799    0.037779   1.1904\n",
      " -0.17714   -0.2595    -0.31461    0.038825  -0.15713   -0.13484\n",
      "  0.36936   -0.30562   -0.40619   -0.38965    0.3686     0.013963\n",
      " -0.6895     0.004066  -0.1367     0.32564    0.24688   -0.14011\n",
      "  0.53889   -0.80441   -0.1777    -0.12922    0.16303    0.14917\n",
      " -0.068429  -0.33922    0.18495   -0.082544  -0.46892    0.39581\n",
      " -0.13742   -0.35132    0.22223   -0.144     -0.048287   0.3379\n",
      " -0.31916    0.20526    0.098624  -0.23877    0.045338   0.43941\n",
      "  0.030385  -0.013821  -0.093273  -0.18178    0.19438   -0.3782\n",
      "  0.70144    0.16236    0.0059111  0.024898  -0.13613   -0.11425\n",
      " -0.31598   -0.14209    0.028194   0.5419    -0.42413   -0.599\n",
      "  0.24976   -0.27003    0.14964    0.29287   -0.31281    0.16543\n",
      " -0.21045   -0.4408     1.2174     0.51236    0.56209    0.14131\n",
      "  0.092514   0.71396   -0.021051  -0.33704   -0.20275   -0.36181\n",
      "  0.22055   -0.25665    0.28425   -0.16968    0.058029   0.61182\n",
      "  0.31576   -0.079185   0.35538   -0.51236    0.4235    -0.30033\n",
      " -0.22376    0.15223   -0.048292   0.23532    0.46507   -0.67579\n",
      " -0.32905    0.08446   -0.22123   -0.045333   0.34463   -0.1455\n",
      " -0.18047   -0.17887    0.96879   -1.0028    -0.47343    0.28542\n",
      "  0.56382   -0.33211   -0.38275   -0.2749    -0.22955   -0.24265\n",
      " -0.37689    0.24822    0.36941    0.14651   -0.37864    0.31134\n",
      " -0.28449    0.36948   -2.8174    -0.38319   -0.022373   0.56376\n",
      "  0.40131   -0.42131   -0.11311   -0.17317    0.1411    -0.13194\n",
      "  0.18494    0.097692  -0.097341  -0.23987    0.16631   -0.28556\n",
      "  0.0038654  0.53292   -0.32367   -0.38744    0.27011   -0.34181\n",
      " -0.27702   -0.67279   -0.10771   -0.062189  -0.24783   -0.070884\n",
      " -0.20898    0.062404   0.022372   0.13408    0.1305    -0.19546\n",
      " -0.46849    0.77731   -0.043978   0.3827    -0.23376    1.0457\n",
      " -0.14371   -0.3565    -0.080713  -0.31047   -0.57822   -0.28067\n",
      " -0.069678   0.068929  -0.16227   -0.63934   -0.62149    0.11222\n",
      " -0.16969   -0.54637    0.49661    0.46565    0.088294  -0.48496\n",
      "  0.69263   -0.068977  -0.53709    0.20802   -0.42987   -0.11921\n",
      "  0.1174    -0.18443    0.43797   -0.1236     0.3607    -0.19608\n",
      " -0.35366    0.18808   -0.5061     0.14455   -0.024368  -0.10772\n",
      " -0.0115     0.58634   -0.054461   0.0076487 -0.056297   0.27193\n",
      "  0.23096   -0.29296   -0.24325    0.10317   -0.10014    0.7089\n",
      "  0.17402   -0.0037509 -0.46304    0.11806   -0.16457   -0.38609\n",
      "  0.14524    0.098122  -0.12352   -0.1047     0.39047   -0.3063\n",
      " -0.65375   -0.0044248 -0.033876   0.037114  -0.27472    0.0053147\n",
      "  0.30737    0.12528   -0.19527   -0.16461    0.087518  -0.051107\n",
      " -0.16323    0.521      0.10822   -0.060379  -0.71735   -0.064327\n",
      "  0.37043   -0.41054   -0.2728    -0.30217    0.015771  -0.43056\n",
      "  0.35647    0.17188   -0.54598   -0.21541   -0.044889  -0.10597\n",
      " -0.54391    0.53908    0.070938   0.097839   0.097908   0.17805\n",
      "  0.18995    0.49962   -0.18529    0.051234   0.019574   0.24805\n",
      "  0.3144    -0.29304    0.54235    0.46672    0.26017   -0.44705\n",
      "  0.28287   -0.033345  -0.33181   -0.10902   -0.023324   0.2106\n",
      " -0.29633    0.81506    0.038524   0.46004    0.17187   -0.29804  ]\n",
      "lion True (300,) [ 1.8963e-01 -4.0309e-01  3.5350e-01 -4.7907e-01 -4.3311e-01  2.3857e-01\n",
      "  2.6962e-01  6.4332e-02  3.0767e-01  1.3712e+00 -3.7582e-01 -2.2713e-01\n",
      " -3.5657e-01 -2.5355e-01  1.7543e-02  3.3962e-01  7.4723e-02  5.1226e-01\n",
      " -3.9759e-01  5.1333e-03 -3.0929e-01  4.8911e-02 -1.8610e-01 -4.1702e-01\n",
      " -8.1639e-01 -1.6908e-01 -2.6246e-01 -1.5983e-02  1.2479e-01 -3.7276e-02\n",
      " -5.7125e-01 -1.6296e-01  1.2376e-01 -5.5464e-02  1.3244e-01  2.7519e-02\n",
      "  1.2592e-01 -3.2722e-01 -4.9165e-01 -3.5559e-01 -3.0630e-01  6.1185e-02\n",
      " -1.6932e-01 -6.2405e-02  6.5763e-01 -2.7925e-01 -3.0450e-03 -2.2400e-02\n",
      " -2.8015e-01 -2.1975e-01 -4.3188e-01  3.9864e-02 -2.2102e-01 -4.2693e-02\n",
      "  5.2748e-02  2.8726e-01  1.2315e-01 -2.8662e-02  7.8294e-02  4.6754e-01\n",
      " -2.4589e-01 -1.1064e-01  7.2250e-02 -9.4980e-02 -2.7548e-01 -5.4097e-01\n",
      "  1.2823e-01 -8.2408e-02  3.1035e-01 -6.3394e-02 -7.3755e-01 -5.4992e-01\n",
      "  9.9999e-02 -2.0758e-01 -3.9674e-02  2.0664e-01 -9.7557e-02 -3.7092e-01\n",
      "  2.7901e-01 -6.2218e-01 -1.0280e-01  2.3271e-01  4.3838e-01  3.2445e-02\n",
      " -2.9866e-01 -7.3611e-02  7.1594e-01  1.4241e-01  2.7770e-01 -3.9892e-01\n",
      "  3.6656e-02  1.5759e-01  8.2014e-02 -5.7343e-01  3.5457e-01  2.2491e-01\n",
      " -6.2699e-01 -8.8106e-02  2.4361e-01  3.8533e-01 -1.4083e-01  1.7691e-01\n",
      "  7.0897e-02  1.7951e-01 -4.5907e-01 -8.2120e-01 -2.6631e-02  6.2549e-02\n",
      "  4.2415e-01 -8.9630e-02 -2.4654e-01  1.4156e-01  4.0187e-01 -4.1232e-01\n",
      "  8.4516e-02 -1.0626e-01  7.3145e-01  1.9217e-01  1.4240e-01  2.8511e-01\n",
      " -2.9454e-01 -2.1948e-01  9.0460e-01 -1.9098e-01 -1.0340e+00 -1.5754e-01\n",
      " -1.1964e-01  4.9888e-01 -1.0624e+00 -3.2820e-01 -1.1232e-02 -7.9482e-01\n",
      "  3.7275e-01 -6.8710e-03 -2.5772e-01 -4.7005e-01 -4.1387e-01 -6.4089e-02\n",
      " -2.8033e-01 -4.0778e-02 -2.4866e+00  6.2494e-03 -1.0210e-02  1.2752e-01\n",
      "  3.4965e-01 -1.2571e-01  3.1570e-01  4.1926e-01  2.0056e-01 -5.5984e-01\n",
      " -2.2801e-01  1.2012e-01 -2.0518e-03 -8.9764e-02 -8.0373e-02  1.1969e-02\n",
      " -2.6978e-01  3.4829e-01  7.3664e-03 -1.1137e-01  6.3410e-01  3.8449e-01\n",
      " -6.2248e-01  4.1145e-02  2.5922e-01  6.5811e-01 -4.9548e-01 -1.3030e-01\n",
      " -3.8279e-01  1.1156e-01 -4.3085e-01  3.4473e-01  2.7109e-02 -2.5108e-01\n",
      " -2.8011e-01  2.1662e-01  3.2660e-01  5.5895e-02  7.6077e-02 -5.2480e-02\n",
      "  4.5928e-02 -2.5266e-01  5.2845e-01 -1.3145e-01 -1.2453e-01  4.0556e-01\n",
      "  3.1877e-01  2.4415e-02 -2.2620e-01 -6.1960e-01 -4.0886e-01 -3.5534e-02\n",
      " -5.5123e-03  2.3438e-01  8.7854e-01 -2.5161e-01  4.0600e-01 -4.4284e-01\n",
      "  3.4934e-01 -5.6429e-01 -2.3676e-01  6.2199e-01 -2.8175e-01  4.2024e-01\n",
      "  1.0043e-01 -1.4720e-01  4.9593e-01 -3.5850e-01 -1.3998e-01 -2.7494e-01\n",
      "  2.3827e-01  5.7268e-01  7.9025e-02  1.7872e-02 -2.1829e-01  5.5050e-02\n",
      " -5.4200e-01  1.6788e-01  3.9065e-01  3.0209e-01  2.3040e-01 -3.9351e-02\n",
      " -2.1078e-01 -2.7224e-01  1.6907e-01  5.4819e-01  9.4888e-02  7.9798e-01\n",
      " -6.6158e-02  1.9844e-01  2.0307e-01  4.4808e-02 -1.0240e-01 -6.9909e-02\n",
      " -3.6756e-02  9.5159e-02 -2.7830e-01 -1.0597e-01 -1.6276e-01 -1.8211e-01\n",
      " -3.1897e-01 -2.1633e-01  1.4994e-01 -7.2057e-02  2.2264e-01 -4.5551e-01\n",
      "  3.0341e-01  1.8431e-01  2.1681e-01 -3.1940e-01  2.6426e-01  5.8106e-01\n",
      "  5.4635e-02  6.3238e-01  4.3169e-01  9.0343e-02  1.9494e-01  3.5483e-01\n",
      " -2.0706e-02 -7.3117e-01  1.2941e-01  1.7418e-01 -1.5065e-01  5.3355e-02\n",
      "  4.4794e-02 -1.6600e-01  2.2007e-01 -5.3970e-01 -2.4968e-01 -2.6464e-01\n",
      " -5.5515e-01  5.8242e-01  2.2295e-01  2.4433e-01  4.5275e-01  3.4693e-01\n",
      "  1.2255e-01 -3.9059e-02 -3.2749e-01 -2.7891e-01  1.3766e-01  3.8392e-01\n",
      "  1.0543e-03 -1.0242e-02  4.9205e-01 -1.7922e-01  4.1215e-02  1.3547e-01\n",
      " -2.0598e-01 -2.3194e-01 -7.7701e-01 -3.8237e-01 -7.6383e-01  1.9418e-01\n",
      " -1.5441e-01  8.9740e-01  3.0626e-01  4.0376e-01  2.1738e-01 -3.8050e-01]\n",
      "ghjg False (300,) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.has_vector, token.vector.shape,token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.80168545\n",
      "dog lion 0.47424486\n",
      "dog ghjg 0.0\n",
      "cat dog 0.80168545\n",
      "cat cat 1.0\n",
      "cat lion 0.52654374\n",
      "cat ghjg 0.0\n",
      "lion dog 0.47424486\n",
      "lion cat 0.52654374\n",
      "lion lion 1.0\n",
      "lion ghjg 0.0\n",
      "ghjg dog 0.0\n",
      "ghjg cat 0.0\n",
      "ghjg lion 0.0\n",
      "ghjg ghjg 1.0\n"
     ]
    }
   ],
   "source": [
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uploading the data\n",
    "\n",
    "df = pd.read_csv('imdb_labelled.txt',sep='\\t', header=None)\n",
    "df.rename(columns={0:'Text',1:'Sentiment'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3       Very little music or anything to speak of.            0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text']=df['Text'].apply(lambda x: x.lower() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "def contraction_(x):\n",
    "    for cont , exp in contraction_mapping.items():\n",
    "        x = re.sub(cont,exp,x)\n",
    "    return x  \n",
    "df['Text']=df['Text'].apply(lambda x: contraction_(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Text']=df['Text'].apply(lambda x : re.sub('[a-zA-Z0-9_-]*@[a-zA-Z0-9_-]+\\.com',' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the best scene in the movie was when gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  a very, very, very slow-moving, aimless movie ...          0\n",
       "1  not sure who was more lost - the flat characte...          0\n",
       "2  attempting artiness with black & white and cle...          0\n",
       "3       very little music or anything to speak of.            0\n",
       "4  the best scene in the movie was when gerardo i...          1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "df['Text']=df['Text'].apply(lambda x : re.sub(regex, '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text']=df['Text'].apply(lambda x : re.sub('[^\\w ]+','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text']=df['Text'].apply(lambda x : re.sub('\\s{2,}',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "df['Text']=df['Text'].apply(lambda x : BeautifulSoup(x,'html').get_text().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def remove_accented_chars(x):\n",
    "    x = unicodedata.normalize('NFKD', x).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "    return x\n",
    "df['Text']=df['Text'].apply(lambda x : remove_accented_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "df['Text']=df['Text'].apply(lambda x : ' '.join([w for w in x.split() if w not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def convert_to_root(x):\n",
    "    doc= nlp(x)\n",
    "    x_list=[]\n",
    "    for w in doc:\n",
    "        root = w.lemma_\n",
    "        x_list.append(root)\n",
    "    return ' '.join(x_list)\n",
    "df['Text']=df['Text'].apply(lambda x : convert_to_root(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    386\n",
       "0    362\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization of the text\n",
    "# understanding of how words are converted into vector and vice versa.\n",
    "# below codes are explaining the mathematics about all the vectors\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('Cricket is my favorite sport').vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = nlp('mango tree')\n",
    "a2 = nlp('mango')\n",
    "a3 = nlp('tree')\n",
    "a5 = nlp('queen')\n",
    "a6 = nlp('princess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding mango and tree seperately, and then compare the result with a1(mango tree)\n",
    "a4 = np.add(a2.vector, a3.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine similarity comes as 1, that means it matches completely\n",
    "\n",
    "cosine_similarity(a1.vector.reshape(1,-1),a4.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6578181]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing queen and princess\n",
    "cosine_similarity(a5.vector.reshape(1,-1),a6.vector.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = nlp('i will come tomorrow.')\n",
    "s2 = nlp(' tomorrow i will see you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9464638]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(s1.vector.reshape(1,-1),s2.vector.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# king - man + woman = queen\n",
    "\n",
    "new_vector = np.add(np.subtract(nlp('king').vector,nlp('man').vector),nlp('woman').vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57388\n"
     ]
    }
   ],
   "source": [
    "counter= 0\n",
    "for w in nlp.vocab:\n",
    "    counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = nlp.vocab['king']\n",
    "man = nlp.vocab['man']\n",
    "woman = nlp.vocab['woman']\n",
    "queen = nlp.vocab['queen']\n",
    "\n",
    "# resulting vector\n",
    "result = king.vector - man.vector + woman.vector\n",
    "\n",
    "# function to compute cosine similarity\n",
    "cosine = lambda v1, v2: np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72526103"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(king.vector,queen.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between queen and result: 0.7880845\n"
     ]
    }
   ],
   "source": [
    "print('Similarity between queen and result:', cosine(result, queen.vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find similar words for a word eg. cat, lion etc\n",
    "def similar(x):\n",
    "    allwords = [w for w in nlp.vocab if w.has_vector and w.is_lower and w.lower_ != x ]\n",
    "    x_n= nlp(x).vector\n",
    "    similar = sorted(allwords, key=lambda w : cosine(x_n,w.vector), reverse = True)\n",
    "    return [w.text for w in similar[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find similar words for a vector eg. 300 dimensional array\n",
    "def similar_w_from_array(x_array):\n",
    "    allwords = [w for w in nlp.vocab if w.has_vector and w.is_lower  ]\n",
    "#     x_n= nlp(x).vector\n",
    "    similar = sorted(allwords, key=lambda w : cosine(x_array,w.vector), reverse = True)\n",
    "    return [w.text for w in similar[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king',\n",
       " 'queen',\n",
       " 'prince',\n",
       " 'kings',\n",
       " 'princess',\n",
       " 'royal',\n",
       " 'throne',\n",
       " 'queens',\n",
       " 'monarch',\n",
       " 'kingdom']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result is an array which was calculated from \n",
    "# result = king - man + woman \n",
    "\n",
    "similar_w_from_array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lions',\n",
       " 'tiger',\n",
       " 'elephant',\n",
       " 'leopard',\n",
       " 'panther',\n",
       " 'lioness',\n",
       " 'wolf',\n",
       " 'bear',\n",
       " 'cheetah',\n",
       " 'hippo']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar('lion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the text into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Vec'] = df['Text'].apply(lambda x : nlp(x).vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>slowmov aimless movie distress drift young man</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.017769285, -0.023674145, -0.13877885, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sure lose flat character audience nearly half ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.116892815, 0.28192127, -0.16150314, 0.09832...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attempt artiness black white clever camera ang...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.30202594, -0.058239333, -0.051761672, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>little music speak</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.12125667, 0.3263667, -0.14098333, 0.314453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good scene movie gerardo try find song keep ru...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.031121409, 0.064626694, -0.144304, -0.09795...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment  \\\n",
       "0     slowmov aimless movie distress drift young man          0   \n",
       "1  sure lose flat character audience nearly half ...          0   \n",
       "2  attempt artiness black white clever camera ang...          0   \n",
       "3                                 little music speak          0   \n",
       "4  good scene movie gerardo try find song keep ru...          1   \n",
       "\n",
       "                                                 Vec  \n",
       "0  [-0.017769285, -0.023674145, -0.13877885, 0.06...  \n",
       "1  [0.116892815, 0.28192127, -0.16150314, 0.09832...  \n",
       "2  [-0.30202594, -0.058239333, -0.051761672, -0.1...  \n",
       "3  [-0.12125667, 0.3263667, -0.14098333, 0.314453...  \n",
       "4  [0.031121409, 0.064626694, -0.144304, -0.09795...  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.Vec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748, 3)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748,)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so there are 748 text with (300,) vector for each.\n",
    "df['Vec'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5], [6]])\n",
    "np.concatenate(a, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748,)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(X, axis=0).reshape(-1,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748, 300)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01776928, -0.02367415, -0.13877885, ...,  0.10296571,\n",
       "        -0.35885713, -0.035865  ],\n",
       "       [ 0.11689281,  0.28192127, -0.16150314, ..., -0.01820625,\n",
       "         0.15033989, -0.068809  ],\n",
       "       [-0.30202594, -0.05823933, -0.05176167, ..., -0.0179723 ,\n",
       "        -0.01303827,  0.06434432],\n",
       "       ...,\n",
       "       [-0.357427  , -0.08321025, -0.112372  , ...,  0.1542    ,\n",
       "        -0.041888  , -0.284315  ],\n",
       "       [-0.56238   ,  0.28869   , -0.32810998, ..., -0.2147    ,\n",
       "         0.096884  , -0.2559385 ],\n",
       "       [-0.5939315 ,  0.0496    ,  0.10329019, ...,  0.03031239,\n",
       "        -0.18185751,  0.11881167]], dtype=float32)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((748,), (748, 300))"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mein Pc\\.conda\\envs\\nlp_course\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=LogisticRegression()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 81,  39],\n",
       "       [ 19, 108]], dtype=int64)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       120\n",
      "           1       0.73      0.85      0.79       127\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       247\n",
      "   macro avg       0.77      0.76      0.76       247\n",
      "weighted avg       0.77      0.77      0.76       247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Re execute it with hyperparameter tuning via GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = {\n",
    "                'penalty': ['l1','l2'],\n",
    "                'C' : (1,2,3,4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_h = GridSearchCV(lr, hyperparameter, n_jobs=-1,cv= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mein Pc\\.conda\\envs\\nlp_course\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': (1, 2, 3, 4)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_h.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_h.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_h.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8403193612774451"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_h.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= clf_h.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74       120\n",
      "           1       0.73      0.85      0.79       127\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       247\n",
      "   macro avg       0.77      0.76      0.76       247\n",
      "weighted avg       0.77      0.77      0.76       247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 81,  39],\n",
       "       [ 19, 108]], dtype=int64)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bravo!! word2vec implementation on IMDB data is done.. with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazypredict library\n",
    "\n",
    "- this is to check for all possible machine learning models and provide the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/36/9e022b76a3ac440e1d750c64fa6152469f988efe0c568b945e396e2693b5/pytest-6.1.1-py3-none-any.whl (272kB)\n",
      "Collecting atomicwrites>=1.0; sys_platform == \"win32\" (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/a0/da5f49008ec6e9a658dbf5d7310a4debd397bce0b4db03cf8a410066bb87/atomicwrites-1.4.0-py2.py3-none-any.whl\n",
      "Collecting toml (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/e1/1b40b80f2e1663a6b9f497123c11d7d988c0919abbf3c3f2688e448c5363/toml-0.10.1-py2.py3-none-any.whl\n",
      "Collecting iniconfig (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/dd/b3c12c6d707058fa947864b67f0c4e0c39ef8610988d7baea9578f3c48f3/iniconfig-1.1.1-py2.py3-none-any.whl\n",
      "Collecting packaging (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\n",
      "Collecting py>=1.8.2 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/68/0f/41a43535b52a81e4f29e420a151032d26f08b62206840c48d14b70e53376/py-1.9.0-py2.py3-none-any.whl (99kB)\n",
      "Collecting pluggy<1.0,>=0.12 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from pytest) (1.5.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from pytest) (0.4.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from pytest) (19.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from packaging->pytest) (2.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from packaging->pytest) (1.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mein pc\\.conda\\envs\\nlp_course\\lib\\site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest) (2.2.0)\n",
      "Installing collected packages: atomicwrites, toml, iniconfig, packaging, py, pluggy, pytest\n",
      "Successfully installed atomicwrites-1.4.0 iniconfig-1.1.1 packaging-20.4 pluggy-0.13.1 py-1.9.0 pytest-6.1.1 toml-0.10.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install lazypredict\n",
    "#!pip install xgboost\n",
    "#!pip install lightgbm\n",
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.7'"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazypredict.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.ensemble' has no attribute '_hist_gradient_boosting'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-287-98e5760a039b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlazypredict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# from sklearn.datasets import load_breast_cancer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# from sklearn.model_selection import train_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# data = load_breast_cancer()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# X = data.data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp_course\\lib\\site-packages\\lazypredict\\Supervised.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[1;34m'GaussianProcessClassifier'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgaussian_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGaussianProcessClassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m  ('HistGradientBoostingClassifier',\n\u001b[1;32m---> 40\u001b[1;33m   sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier),\n\u001b[0m\u001b[0;32m     41\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[1;34m'MLPClassifier'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultilayer_perceptron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMLPClassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[1;34m'LogisticRegressionCV'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogisticRegressionCV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn.ensemble' has no attribute '_hist_gradient_boosting'"
     ]
    }
   ],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# data = load_breast_cancer()\n",
    "# X = data.data\n",
    "# y= data.target\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.5,random_state =123)\n",
    "# clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "# models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
